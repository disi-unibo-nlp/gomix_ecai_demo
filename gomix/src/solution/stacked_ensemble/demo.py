import os
import sys
from pathlib import Path
import json
from itertools import chain
from typing import List, Tuple
import random
import torch
sys.path.append(str(Path(__file__).resolve().parents[3]))
from src.solution.components.naive.NaiveLearner import NaiveLearner
from src.solution.components.diamondscore.DiamondScoreLearner import DiamondScoreLearner
from src.solution.components.interactionscore.InteractionScoreLearner import InteractionScoreLearner
from src.solution.components.embeddingsimilarityscore.main import EMBEDDING_TYPES as EMBEDDING_TYPES_FOR_EMBEDDING_SIMILARITY_SCORE
from src.solution.components.embeddingsimilarityscore.Learner import Learner as EmbeddingSimilarityScoreLearner
from src.solution.components.FC_on_embeddings.main import make_and_train_model_on as make_and_train_fc_on_embeddings_model, \
    make_model_on_device as make_fc_on_embeddings_model, \
    make_training_dataset_with_annotations as make_fc_training_dataset, \
    predict_and_transform_predictions_to_dict as predict_with_nn_model_and_transform_preds_to_dict
from src.solution.components.GNN_on_PPI_with_embeddings.main import build_whole_graph_from_scratch, \
    make_and_train_model_on as make_and_train_gnn_model, \
    make_model_on_device as make_gnn_model, \
    predict_and_transform_predictions_to_dict as predict_with_gnn_model_and_transform_preds_to_dict
from src.solution.stacked_ensemble.StackingMetaLearner import StackingMetaLearner
from src.solution.stacked_ensemble.Level1Dataset import Level1Dataset
from src.utils.ProteinEmbeddingLoader import ProteinEmbeddingLoader
from src.utils.predictions_evaluation.evaluate import evaluate_with_deepgoplus_evaluator

TASK_DATASET_PATH = os.environ["TASK_DATASET_PATH"]
assert TASK_DATASET_PATH, 'Environment variable \'TASK_DATASET_PATH\' must be declared.'

GENE_ONTOLOGY_FILE_PATH = os.path.join(TASK_DATASET_PATH, 'go.obo')
ALL_PROTEINS_DIAMOND_SCORES_FILE_PATH = os.path.join(TASK_DATASET_PATH, 'all_proteins_diamond.res')
PPI_FILE_PATH = os.path.join(TASK_DATASET_PATH, 'all_proteins_STRING_interactions.json')

CHECKPOINTS_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../data/temp_cache/model_checkpoints')

USE_ALL_COMPONENTS = False  # When this is False, only the simpler base learners will be used.


"""
Example usage:
TASK_DATASET_PATH=data/processed/task_datasets/2016 python src/solution/stacked_ensemble/demo.py
"""
def main():
    train_annotations_file_path = os.path.join(TASK_DATASET_PATH, 'propagated_annotations', 'train.json')
    test_annotations_file_path = os.path.join(TASK_DATASET_PATH, 'annotations', 'test.json')

    random.seed(0)
    torch.manual_seed(0)

    with open(train_annotations_file_path, 'r') as f:
        train_annotations = json.load(f)  # dict: prot ID -> list of GO terms

    with open(test_annotations_file_path, 'r') as f:
        test_annotations = json.load(f)  # dict: prot ID -> list of GO terms

    print(f'USE_ALL_COMPONENTS is set to {USE_ALL_COMPONENTS}. This means that the GNN and NN models will {"be" if USE_ALL_COMPONENTS else "NOT be"} used in the ensemble.')
    print('Train set | total proteins:', len(train_annotations), '| total GO terms:', len(set(chain.from_iterable(train_annotations.values()))))
    print('Test set | total proteins:', len(test_annotations), '| total GO terms:', len(set(chain.from_iterable(test_annotations.values()))))

    train_go_terms_vocabulary = create_go_terms_vocabulary_from_annotations(train_annotations)
    meta_learner = StackingMetaLearner(n_classes=len(train_go_terms_vocabulary))

    """
    Split training set into train and target sets.
    The base models will be trained on the train set, and their predictions
    will be generated for the target set. The meta-learner will be trained
    on the target set, with inputs generated by the base models.
    """
    base_models_train_annotations, meta_learner_train_annotations = random_split_dict(train_annotations, split_percentage=0.8)

    print('\nPreparing data for training the meta-learner...')
    print(f'Training the base models on {len(base_models_train_annotations)} proteins, in order to then generate level-1 predictions for {len(meta_learner_train_annotations)} proteins...')
    base_models_predictions = train_base_models_and_generate_level1_predictions(
        train_annotations=base_models_train_annotations,
        target_prot_ids=list(meta_learner_train_annotations.keys())
    )
    print('Level-1 train dataset generation completed.')

    # Train the meta-learner.
    level1_train_dataset = Level1Dataset(
        go_terms_vocabulary=train_go_terms_vocabulary,
        base_predictions=base_models_predictions,
        ground_truth=meta_learner_train_annotations,
    )
    print(f'Training the meta-learner on {len(meta_learner_train_annotations)} proteins...')
    meta_learner.fit(
        base_scores=level1_train_dataset.get_base_scores_array(),
        labels=level1_train_dataset.get_labels_array()
    )
    print('Finished training the meta-learner.')

    # ------ Test the ensemble ------ #

    print('\n------\n')
    print('Will now test the ensemble. But before that, will re-train the base models on the whole training set...')

    test_base_predictions = train_base_models_and_generate_level1_predictions(
        train_annotations=train_annotations,  # Re-training the base models on the whole training set
        target_prot_ids=list(test_annotations.keys())
    )
    print('\nPreparing level-1 test dataset...')
    level1_test_dataset = Level1Dataset(
        go_terms_vocabulary=train_go_terms_vocabulary,
        base_predictions=test_base_predictions,
    )

    final_predictions = meta_learner.predict(level1_test_dataset.get_base_scores_array())
    final_predictions = level1_test_dataset.convert_predictions_array_to_dict(final_predictions)

    print('\nTest results:')
    evaluate_with_deepgoplus_evaluator(
        gene_ontology_file_path=GENE_ONTOLOGY_FILE_PATH,
        predictions=final_predictions,
        ground_truth=test_annotations
    )


def random_split_dict(dictionary: dict, split_percentage: float) -> Tuple[dict, dict]:
    if split_percentage < 0 or split_percentage > 1:
        raise ValueError('split_percentage should be in the range [0, 1]')

    dict_items = list(dictionary.items())
    random.shuffle(dict_items)
    num_items = len(dict_items)
    split_idx = int(num_items * split_percentage)

    left_set = dict(dict_items[:split_idx])
    right_set = dict(dict_items[split_idx:])

    return left_set, right_set


def create_go_terms_vocabulary_from_annotations(annotations: dict) -> List[str]:
    return list(set(chain.from_iterable(annotations.values())))


def train_base_models_and_generate_level1_predictions(train_annotations: dict, target_prot_ids: List[str]) -> List[dict]:
    """
    Prepare the base models.
    """
    naive_learner = NaiveLearner(train_annotations)
    diamondscore_learner = DiamondScoreLearner(train_annotations, ALL_PROTEINS_DIAMOND_SCORES_FILE_PATH)
    interactionscore_learner = InteractionScoreLearner(train_annotations, PPI_FILE_PATH)
    embeddingsimilarityscore_learner = EmbeddingSimilarityScoreLearner(train_annotations=train_annotations, prot_embedding_loader=ProteinEmbeddingLoader(EMBEDDING_TYPES_FOR_EMBEDDING_SIMILARITY_SCORE))

    if USE_ALL_COMPONENTS:
        # The trained models will be saved to disk, so that they can be re-used later in this function.
        # Why not keeping them in memory? Because they're too big to keep them all in memory at once.
        go_term_to_nn_output_index, = train_neural_fc_on_embeddings(train_annotations)
        graph, graph_ctx = train_gnn_model_with_annotations(train_annotations)

    """
    Generate predictions (without overloading memory).
    """

    predictions = [
        {prot_id: [(go_term, score) for go_term, score in naive_learner.predict().items()] for prot_id in target_prot_ids},
        {prot_id: [(go_term, score) for go_term, score in diamondscore_learner.predict(prot_id).items()] for prot_id in target_prot_ids},
        {prot_id: [(go_term, score) for go_term, score in interactionscore_learner.predict(prot_id).items()] for prot_id in target_prot_ids},
        {prot_id: [(go_term, score) for go_term, score in embeddingsimilarityscore_learner.predict(prot_id).items()] for prot_id in target_prot_ids},
    ]

    if USE_ALL_COMPONENTS:
        nn_model = make_fc_on_embeddings_model(go_term_to_nn_output_index)
        load_checkpoint(model=nn_model, model_name='fc_on_embeddings')
        predictions.append(
            predict_with_nn_model_and_transform_preds_to_dict(model=nn_model, prot_ids=target_prot_ids, go_term_to_index=go_term_to_nn_output_index)
        )
        del nn_model

        gnn_model = make_gnn_model(graph_ctx=graph_ctx)
        load_checkpoint(model=gnn_model, model_name='gnn')
        predictions.append(
            predict_with_gnn_model_and_transform_preds_to_dict(model=gnn_model, prot_ids=target_prot_ids, graph=graph, graph_ctx=graph_ctx)
        )
        del gnn_model

    return predictions


def train_neural_fc_on_embeddings(train_annotations) -> Tuple[dict]:
    print(f"\nAbout to train the NN model on {len(train_annotations)} proteins.")
    dataset = make_fc_training_dataset(train_annotations)
    model = make_and_train_fc_on_embeddings_model(dataset)
    save_checkpoint(model=model, model_name='fc_on_embeddings')
    return dataset.go_term_to_index,


def train_gnn_model_with_annotations(train_annotations) -> Tuple[any, dict]:
    print(f"\nAbout to train the GNN model on {len(train_annotations)} proteins.")
    graph, graph_ctx = build_whole_graph_from_scratch(train_annotations)
    model = make_and_train_gnn_model(graph=graph, graph_ctx=graph_ctx)
    save_checkpoint(model=model, model_name='gnn')
    return graph, graph_ctx


def save_checkpoint(model: torch.nn.Module, model_name: str):
    if not os.path.exists(CHECKPOINTS_DIR):
        os.makedirs(CHECKPOINTS_DIR)
    path = os.path.join(CHECKPOINTS_DIR, f'{model_name}.pt')
    torch.save(model.state_dict(), path)


def load_checkpoint(model: torch.nn.Module, model_name: str):
    path = os.path.join(CHECKPOINTS_DIR, f'{model_name}.pt')
    model.load_state_dict(torch.load(path))


if __name__ == '__main__':
    main()
